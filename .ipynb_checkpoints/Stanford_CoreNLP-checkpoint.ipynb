{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "import io\n",
    "import re\n",
    "import json\n",
    "import boto3\n",
    "import pickle\n",
    "import stanza\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:Configure s3...\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"bucket\"] = 'frus-corenlp'\n",
    "\n",
    "# Configure s3 client and resource\n",
    "logging.critical(\"Configure s3...\")\n",
    "bucket = os.environ[\"bucket\"]\n",
    "# s3 = boto3.client('s3', aws_access_key_id=aws_id, aws_secret_access_key=aws_secret_key)\n",
    "# s3_resource = boto3.resource('s3', aws_access_key_id=aws_id, aws_secret_access_key=aws_secret_key)\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "s3_resource = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 3.60MB/s]\n",
      "2020-10-07 16:40:05 INFO: Downloading default packages for language: en (English)...\n",
      "INFO:stanza:Downloading default packages for language: en (English)...\n",
      "Downloading http://nlp.stanford.edu/software/stanza/1.1.0/en/default.zip: 100%|█████| 428M/428M [06:06<00:00, 1.17MB/s]\n",
      "2020-10-07 16:46:17 INFO: Finished downloading models and saved to ..\n",
      "INFO:stanza:Finished downloading models and saved to ..\n"
     ]
    }
   ],
   "source": [
    "# stanza.download('en', '.') # download English model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-07 16:46:17 INFO: Installing CoreNLP package into C:\\Users\\desid\\stanza_corenlp...\n",
      "INFO:stanza:Installing CoreNLP package into C:\\Users\\desid\\stanza_corenlp...\n",
      "Downloading http://nlp.stanford.edu/software/stanford-corenlp-latest.zip: 100%|██████| 505M/505M [10:01<00:00, 839kB/s]\n"
     ]
    }
   ],
   "source": [
    "# stanza.install_corenlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Resources file not found at: .\\resources.json. Try to download the model again.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-436cb38aefce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstanza\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\desid\\anaconda3\\lib\\site-packages\\stanza\\pipeline\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, lang, dir, package, processors, logging_level, verbose, use_gpu, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0mresources_filepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'resources.json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresources_filepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Resources file not found at: {resources_filepath}. Try to download the model again.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresources_filepath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[0mresources\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Resources file not found at: .\\resources.json. Try to download the model again."
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline('en', '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_modules():\n",
    "    import os, sys\n",
    "\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    import io\n",
    "    import re\n",
    "    import json\n",
    "    import boto3\n",
    "    import pickle\n",
    "    import stanza\n",
    "    import logging\n",
    "    import pandas as pd\n",
    "    from pymongo import MongoClient\n",
    "    \n",
    "    os.environ[\"bucket\"] = 'frus-corenlp'\n",
    "\n",
    "    # Configure s3 client and resource\n",
    "    logging.critical(\"Configure s3...\")\n",
    "    bucket = os.environ[\"bucket\"]\n",
    "    # s3 = boto3.client('s3', aws_access_key_id=aws_id, aws_secret_access_key=aws_secret_key)\n",
    "    # s3_resource = boto3.resource('s3', aws_access_key_id=aws_id, aws_secret_access_key=aws_secret_key)\n",
    "\n",
    "    s3 = boto3.client('s3')\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    nlp = stanza.Pipeline('en', '.')\n",
    "    \n",
    "    return start_time, bucket, s3, s3_resource, nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_s3(df, key):\n",
    "    '''\n",
    "    This function takes a pandas dataframe, converts it to a csv file, and then uploads it to an s3 bucket.\n",
    "\n",
    "    Parameters:\n",
    "    ---------------\n",
    "    df: pandas dataframe, the dataframe to be saved\n",
    "    s3: authorized boto s3 resource\n",
    "    bucket: str, bucket to upload file to (topic-modeling-deployment)\n",
    "    key: key of the file to be uploaded (.csv file)\n",
    "\n",
    "    Returns:\n",
    "    ---------------\n",
    "    key: str, the key / name of the file in the s3 bucket\n",
    "    '''\n",
    "    csv_buffer = io.StringIO()\n",
    "    df.to_csv(csv_buffer)        \n",
    "    try:\n",
    "        s3_resource.Object(bucket, key).delete()\n",
    "    except:\n",
    "        pass\n",
    "    s3_resource.Object(bucket, key).put(Body=csv_buffer.getvalue())\n",
    "    return key\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_doc(doc):\n",
    "    \n",
    "    # Extract document information\n",
    "    file_id = doc.id \n",
    "    text = doc.text\n",
    "    title = doc.title\n",
    "    url = doc.url\n",
    "    date = doc.date\n",
    "    president = doc.website\n",
    "    batch = doc.batch\n",
    "        \n",
    "    try:\n",
    "        # Only keep first 6 sentences of text\n",
    "        num_sentences = 6\n",
    "        text = ''.join(re.split(r'(?<=[.])(\\s[A-Z])', text)[:num_sentences*2 - 1])[:1000]\n",
    "    \n",
    "        # Feed text into CoreNLP\n",
    "        parsed = nlp(text).to_dict()\n",
    "        \n",
    "        return parsed[0]\n",
    "    #     # Upload results to s3\n",
    "    #     key = f\"parsed/{file_id}.json\"\n",
    "        \n",
    "    #     results = {\n",
    "    #             \"president\": president,\n",
    "    #             \"batch\": int(batch),\n",
    "    #             \"file_id\": int(file_id),\n",
    "    #             \"results\": json.dumps(parsed[0]),\n",
    "    #             \"key\": key,\n",
    "                \n",
    "    #             \"source\": president,\n",
    "    #             \"context\":text,\n",
    "    #             \"url\": url,\n",
    "    #             \"title\": title,\n",
    "    #             \"date\": date,\n",
    "    #             \"stanford\": 0\n",
    "    #           }\n",
    "    \n",
    "    #     # print(results)\n",
    "    #     resp = s3_resource.Object(bucket, key).put(Body=json.dumps(results))\n",
    "    except:\n",
    "        return 'error'\n",
    "    \n",
    "    # # Store in MongoDB\n",
    "    # try:\n",
    "    #     collection.insert_one(results)\n",
    "    # except:\n",
    "    #     return 'duplicate'\n",
    "    \n",
    "\n",
    "    return resp\n",
    "    \n",
    "\n",
    "\n",
    "def handler(event, context):\n",
    "    print(\"Enter handler.\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Get key name\n",
    "    key = event[\"key\"]\n",
    "    timeout = event[\"timeout\"]\n",
    "    batch_name = key.split('/')[-1][:-4]\n",
    "    # logging.critical(\"Parsing {}\".format(batch_name))\n",
    "    print(\"Parsing {}\".format(batch_name))\n",
    "    \n",
    "    # print(\"Loading NLP...\")\n",
    "    # global nlp\n",
    "    # nlp = stanza.Pipeline('en', '/mnt/efs/')\n",
    "    # obj = s3.get_object(Bucket=bucket, Key='nlp')\n",
    "    # data = obj['Body'].read()\n",
    "    # nlp = pickle.loads(data)\n",
    "\n",
    "\n",
    "    # Get file containing documents\n",
    "    data = s3_resource.Object(\"frus-corenlp\", key).get()['Body'].read()\n",
    "    df = pd.read_csv(io.BytesIO(data)).iloc[:, 1:]\n",
    "    if 'parsed' not in df.columns:\n",
    "        df['parsed'] = None\n",
    "    \n",
    "    # Check how many documents need to be parsed\n",
    "    # logging.critical(\"{:,}/{:,} documents need to be parsed.\".format(df.parsed.isnull().sum(), len(df)))\n",
    "    print(\"{:}: {:,}/{:,} documents need to be parsed.\".format(batch_name, df.parsed.isnull().sum(), len(df)))\n",
    "    df.parsed = df.parsed.astype(str)\n",
    "    try:\n",
    "        start_index = df.parsed.to_list().index('nan')\n",
    "    except:\n",
    "        start_index = 0\n",
    "\n",
    "    success = len(df[~df.parsed.isin(['error', 'nan'])])\n",
    "    error_ids = []\n",
    "    dnf_ids = []\n",
    "    # logging.critical(\"Begin parsing loop.\")\n",
    "    print(\"{}: Begin parsing loop.\".format(batch_name))\n",
    "    size = df.shape[0]\n",
    "    for i in range(start_index, size):\n",
    "        # Check if Lambda time is close to timing out\n",
    "        if time.time() - start_time > timeout - 45:\n",
    "            # logging.critical(\"Out of time.\")\n",
    "            print(\"{}: Out of time.\".format(batch_name))\n",
    "            dnf_ids .extend(df.id.tolist()[i:])\n",
    "            break\n",
    "            \n",
    "        # Get document\n",
    "        doc = df.iloc[i]\n",
    "        \n",
    "        # Check document has not already been analyzed\n",
    "        if doc.parsed  == \"nan\":\n",
    "            # logging.critical(\"File {}/{}: now running\".format(i, size))\n",
    "            print(\"{}: File {}/{}: now running\".format(batch_name, i, size))\n",
    "            # Pass doc to CoreNLP\n",
    "            parsed = parse_doc(doc)\n",
    "        \n",
    "            # Check for errors\n",
    "            if parsed == 'error':\n",
    "                error_ids.append(int(doc.id))\n",
    "                df.iloc[i, -1] = 'error'\n",
    "            else:\n",
    "                df.iloc[i, -1] = json.dumps(parsed)\n",
    "                success += 1\n",
    "            \n",
    "            if not (i - start_index + 1) % 10:\n",
    "                # Save progress to s3\n",
    "                df_to_s3(df, key)\n",
    "        \n",
    "        else:\n",
    "            # logging.critical(\"File {}/{} already done\".format(i, size))\n",
    "            print(\"{}: File {}/{} already done\".format(batch_name, i, size))\n",
    "\n",
    "    # Save progress to s3\n",
    "    df_to_s3(df, key)\n",
    "\n",
    "    if success == size:\n",
    "        # logging.critical(\"All documents in batch have been analyzed.\")\n",
    "        print(\"{}: All documents in batch have been analyzed.\".format(batch_name))\n",
    "    \n",
    "        # Save batch dataframe to s3\n",
    "        df_key = 'parsed/{}.csv'.format(batch_name)\n",
    "        df_to_s3(df, df_key)\n",
    "            \n",
    "    message = \"{0:,} documents successfully parsed.\".format(success)\n",
    "    message += \"\\n{0:,} documents raised errors.\".format(len(error_ids))\n",
    "    # message += \"\\n{0:,} documents did not get parsed due to time.\".format(len(dnf_ids))\n",
    "    # logging.critical(message)\n",
    "    print(\"{}:\".format(batch_name), message)\n",
    "    print(\"{}: Time: {:.2f}\".format(batch_name, time.time() - start_time))\n",
    "\n",
    "    return {\"message\": message,\n",
    "            \"error_ids\": json.dumps(error_ids),\n",
    "            \"dnf_ids\": json.dumps(dnf_ids)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'handler'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-114fc1464b4f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmultiprocessing\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mhandler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'handler'"
     ]
    }
   ],
   "source": [
    "from itertools import count\n",
    "from multiprocess import Process, Pool\n",
    "\n",
    "import multiprocessing as mp\n",
    "import handler\n",
    "\n",
    "num_workers = mp.cpu_count() \n",
    "print(num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\desid\\\\Documents\\\\UDel'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.dirname(os.path.realpath('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'handler'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-31ecc537580b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mhandler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'handler'"
     ]
    }
   ],
   "source": [
    "def func(a, b):\n",
    "    time.sleep(5)\n",
    "    return\n",
    "\n",
    "import handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'handler'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-70510a92b757>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#     ps[b] = Process(target=handler, args=(event, context,))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mevent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'handler'"
     ]
    }
   ],
   "source": [
    "timeout = 30\n",
    "context = {}\n",
    "\n",
    "num_threads = 1\n",
    "ps = [None] * mp.cpu_count() \n",
    "for b in range(len(ps)):\n",
    "    key = \"dataframes/batch-{:04d}.csv\".format(b+2)\n",
    "    event = {\"key\": key, \"timeout\":timeout}\n",
    "\n",
    "#     ps[b] = Process(target=handler, args=(event, context,))\n",
    "    ps[b] = Pool(handler.handler, [event, context])\n",
    "    ps[b].start()\n",
    "\n",
    "for b in range(len(ps)):\n",
    "    ps[b].join(timeout=timeout+10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Process name='Process-57' pid=6928 parent=26128 stopped exitcode=1>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
